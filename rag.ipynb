{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Knowledge-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # it's pymupdf library\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "pdf_path = \"/home/ai/TAC2-lbz/MES5448_MES7048_user_manual_8_4_0_7_en.pdf\"\n",
    "\n",
    "# by this function our text would be cleaner and better for our LLM\n",
    "def text_formatter(text:str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# now for opening the pdf and reading it we want this function:\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4, # 1 token is about 4 words. this will be need for passing to LLM\n",
    "                                \"text\": text })\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "# pages_and_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataFrames from our pages and texts\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token count is important because we can't use embedding models with infinite tokens, and also LLMs. so fo choosing best embedding model and best LLM we have to know token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets chunk our large sentences into smaller one\n",
    "\n",
    "splitting sentences in group of 10 sentences \n",
    "\n",
    "it's called text splitting and libraries like **LangChain** can do this for us\n",
    "\n",
    "Goal of doing this is to be more easier to filter our sentences and also much easier for our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the group size\n",
    "num_sentences_chunk_size = 10\n",
    "\n",
    "def split_list(input_list:list[str], slice_size:int=num_sentences_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i: i + slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(item[\"sentences\"])\n",
    "    item[\"number_of_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see what we are doing :D\n",
    "\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to have each chunk as a dictionary item  not in as a list of sentence chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # re is a python library and stands for regex. regex also stands for regular expression XD\n",
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        # join sentences together into a paragraph\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # '.A' -> ', A'\n",
    "        \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in (joined_sentence_chunk.split(\" \"))])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1  token has 4 chars\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "        \n",
    "len(pages_and_chunks) # to see how many pages and chunks we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets filter the dataFrame for rows under the 30 tokens. because they are not much useful and they didn't help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 30\n",
    "pages_and_chunks_over_min_token_length = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_length[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks_over_min_token_length)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding our text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see what is embeddings and why we are using: https://vickiboykis.com/what_are_embeddings/\n",
    "\n",
    "all-mpnet-base-v2 model : https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# this would be our embedding model:\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.to(\"cuda\") # uding gpu for faster embedding \n",
    "\n",
    "for item in tqdm(pages_and_chunks_over_min_token_length):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_length, k=1) # see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.shape(pages_and_chunks_over_min_token_length[100][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embeddings to a file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_length)\n",
    "embedding_df_path= \"/home/ai/TAC2-lbz/text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embedding_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reading csv\n",
    "text_chunks_and_embeddings_df_load = pd.read_csv(embedding_df_path)\n",
    "text_chunks_and_embeddings_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG   Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\" # if gpu is available we choose it and if not we chose cpu as our device\n",
    "\n",
    "# importing text and embeddings\n",
    "text_chunks_and_embeddings_df = pd.read_csv(\"/home/ai/TAC2-lbz/text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# now converting embedding column to a np.array\n",
    "text_chunks_and_embeddings_df[\"embedding\"] = text_chunks_and_embeddings_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# converting embedding into a torch.tensor\n",
    "embeddings = torch.tensor(np.stack(text_chunks_and_embeddings_df[\"embedding\"].tolist(), axis=0), dtype=torch.float32).to(device=device)\n",
    "\n",
    "# converting text and embeddings to the list of dictionaries\n",
    "pages_and_chunks = text_chunks_and_embeddings_df.to_dict(orient=\"records\")\n",
    "\n",
    "text_chunks_and_embeddings_df # to see what i just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape # just seeing what i have created :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device) # embedding our query with same model as we embedded our knowledge-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the query\n",
    "query = \"What is the firmware version synchronized with Version 4.0 of the MES5448 and MES7048 operation manual?\"\n",
    "print(f\"query: {query}\")\n",
    "\n",
    "# embed the query\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# similarity scores\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "\n",
    "# Getting top-k results\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks[382]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi # to check how much gpu memory is available for choosing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM model that i chose : Gemma-7b-it https://huggingface.co/google/gemma-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:56<00:00, 14.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# quantization config \n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "\n",
    "model_id = \"google/gemma-7b-it\" \n",
    "\n",
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 17 08:28:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   32C    P8              22W / 370W |  24236MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3951      C   /usr/bin/python3                            428MiB |\n",
      "|    0   N/A  N/A   3546011      C   /home/ai/TAC2-lbz/bin/python              23800MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      "<bos><start_of_turn>user\n",
      "give me a description about MES5448<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"give me a description about MES5448\"\n",
    "\n",
    "# prompt\n",
    "\n",
    "template = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text}\n",
    "]\n",
    "\n",
    "prompt= tokenizer.apply_chat_template(conversation=template, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"prompt: \\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,  20346,    682,    476,   5966,   1105,  69841, 235308, 235310,\n",
       "         235310, 235321, 235280,    578,  69841, 235308, 235310, 235310, 235321,\n",
       "         235305, 235265,    109,    688,  41018, 235308, 235310, 235310, 235321,\n",
       "         235280,    688,    109,    651,  69841, 235308, 235310, 235310, 235321,\n",
       "         235280,    603,    476,   3178, 235290,   7511, 235269,   3178, 235290,\n",
       "          38943, 235269,   3821, 235290,  29659, 205940,    675,  17295, 182939,\n",
       "           1582,    685, 108657, 235269,  53751, 235269,    578,  12345,  41742,\n",
       "         235265,   1165,   5119,    476, 235248, 235274, 235318, 235290,   2428,\n",
       "          45000, 235288,  22638, 235269,    476,  26168,  16333,   6884,   1812,\n",
       "         235269,    578,    476,   5396,   3001,    576, 182939, 235265,    714,\n",
       "          69841, 235308, 235310, 235310, 235321, 235280,    603,   1578, 235290,\n",
       "         154199,    604,    476,   8080,    576,   3178, 235290,   7511,  32982,\n",
       "           8557, 235269,   3359, 235292,    109, 235287,  39750,   9228,   5188,\n",
       "            108, 235287,   6649,   4815,   5188,    108, 235287,  17494,   2582,\n",
       "           5188,    108, 235287,  31613,  36147,    109,    688,  41018, 235308,\n",
       "         235310, 235310, 235321, 235305,    688,    109,    651,  69841, 235308,\n",
       "         235310, 235310, 235321, 235305,    603,    476,   1536, 235290,  33402,\n",
       "         235269,   3178, 235290,   7511, 205940,    675,  17295, 182939,   1582,\n",
       "            685, 108657, 235269,  53751, 235269,    578,  12345,  41742, 235265,\n",
       "           1165,   5119,    476, 235248, 235274, 235318, 235290,   2428,  45000,\n",
       "         235288,  22638, 235269,    476,  26168,  16333,   6884,   1812, 235269,\n",
       "            578,    476,   5396,   3001,    576, 182939, 235265,    714,  69841,\n",
       "         235308, 235310, 235310, 235321, 235305,    603,   1578, 235290, 154199,\n",
       "            604,    476,   8080,    576,   1536, 235290,  33402,  32982,   8557,\n",
       "         235269,   3359, 235292,    109, 235287,  13027,   9402,  10310,    108,\n",
       "         235287,   8092,  10310,    108, 235287,   7572,   5188,    108, 235287,\n",
       "          91077,    109,    688,   2469,  64017,   1865,  69841, 235308, 235310,\n",
       "         235310, 235321, 235280,    578,  69841, 235308, 235310, 235310, 235321,\n",
       "         235305,  66058,    109, 235287,   5231,  26300,  66058,    714,  69841,\n",
       "         235308, 235310, 235310, 235321, 235305,    919]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\") # failed when using gpu\n",
    "\n",
    "output = llm_model.generate(**input_ids, max_new_tokens=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_output = tokenizer.decode(output[0])\n",
    "\n",
    "text_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAC2-lbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
